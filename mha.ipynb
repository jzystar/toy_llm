{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        print(f\"key shape : {keys.shape}\")\n",
    "        print(f\"queries shape : {queries.shape}\")\n",
    "        print(f\"values shape : {values.shape}\")\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        print(f\"attn_scores shape : {attn_scores.shape}\")\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        print(f\"attn_weights shape : {attn_weights.shape}\")\n",
    "        context_vec = attn_weights @ values\n",
    "        print(f\"context_vec shape : {context_vec.shape}\")\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]                                                   #A\n",
    "d_in = inputs.shape[1]                                    #B\n",
    "d_out = 2\n",
    "print(d_in, d_out)\n",
    "print(inputs[1])\n",
    "print(inputs)\n",
    "print(x_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key shape : torch.Size([6, 2])\n",
      "queries shape : torch.Size([6, 2])\n",
      "values shape : torch.Size([6, 2])\n",
      "attn_scores shape : torch.Size([6, 6])\n",
      "attn_weights shape : torch.Size([6, 6])\n",
      "context_vec shape : torch.Size([6, 2])\n",
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        print(f\"dim_in={d_in}, dim_out={d_out}, context_len={context_length}, dropout rate={dropout}, qkv_bias={qkv_bias}\")\n",
    "        self.query_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.key_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.value_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        # use buffer，因为mask不是parameters，所以当模型model.to(\"cuda\")时不会加载到gpu中，且用buffer可以让mask出现在stat_dict中保存起来，后续load时，若mask有改动可以直接把mask load出来\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_token, dim = x.shape\n",
    "        print(f\"batch_size = {b}, num_token： {num_token}, dim={dim}\")\n",
    "        query = self.query_w(x)\n",
    "        key = self.key_w(x)\n",
    "        value = self.value_w(x)\n",
    "        print(f\"key shape: {key.shape}\")\n",
    "\n",
    "        attention_scores = query @ key.transpose(1,2)\n",
    "\n",
    "        attention_scores.masked_fill_(self.mask.bool(), -torch.inf)\n",
    "\n",
    "        attention_weight = torch.softmax(attention_scores/key.shape[-1]**0.5, dim=-1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        context_vec = attention_weight @ value\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "dim_in=3, dim_out=2, context_len=6, dropout rate=0.0, qkv_bias=False\n",
      "batch_size = 2, num_token： 6, dim=3\n",
      "key shape: torch.Size([2, 6, 2])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, heads_num, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(dim_out % heads_num == 0)\n",
    "        self.dim_out = dim_out\n",
    "        self.head_dim = dim_out // heads_num\n",
    "        self.heads_num = heads_num\n",
    "        self.query_w = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.key_w = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.value_w = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        self.proj_out = nn.Linear(dim_out, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, tokens_num, dim = x.shape\n",
    "        print(f\"input shape: {x.shape}\")\n",
    "        # 把最后dim_out维度打散成head_num x head_dim，然后交换tokens_num和head_num位置便于后面的attention计算\n",
    "        query = self.query_w(x).view(b, tokens_num, self.heads_num, self.head_dim).transpose(1, 2)\n",
    "        key = self.key_w(x)\n",
    "        print(f\"key shape: {key.shape}\")\n",
    "        key = key.view(b, tokens_num, self.heads_num, self.head_dim)\n",
    "        print(f\"key shape after unsqueeze: {key.shape}\")\n",
    "        key = key.transpose(1, 2)\n",
    "        print(f\"key shape after transpose: {key.shape}\")\n",
    "        value = self.value_w(x).view(b, tokens_num, self.heads_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_scores = query @ key.transpose(2, 3)\n",
    "        print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "        # 后面带下划线_的method会改变本身的值\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:tokens_num, :tokens_num], -torch.inf)\n",
    "\n",
    "        attention_weight = torch.softmax(attention_scores / key.shape[-1] ** 0.5, dim=-1)\n",
    "\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        context_vecs = attention_weight @ value\n",
    "        # print(f\"context_vecs shape: {context_vecs.shape}\")\n",
    "        context_vecs = context_vecs.transpose(1, 2)\n",
    "        # print(f\"context_vecs shape after transpose: {context_vecs.shape}\")\n",
    "        context_vecs = context_vecs.contiguous().view(b, tokens_num, self.dim_out)\n",
    "        print(f\"context_vecs shape after squeeze: {context_vecs.shape}\")\n",
    "        context_vecs = self.proj_out(context_vecs)\n",
    "        return context_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_in 3, d_out 2, batch_size 2, context_length 6\n",
      "input shape: torch.Size([2, 6, 3])\n",
      "key shape: torch.Size([2, 6, 2])\n",
      "key shape after unsqueeze: torch.Size([2, 6, 2, 1])\n",
      "key shape after transpose: torch.Size([2, 2, 6, 1])\n",
      "attention_scores shape: torch.Size([2, 2, 6, 6])\n",
      "context_vecs shape: torch.Size([2, 2, 6, 1])\n",
      "context_vecs shape after transpose: torch.Size([2, 6, 2, 1])\n",
      "context_vecs shape after squeeze: torch.Size([2, 6, 2])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "print(f\"d_in {d_in}, d_out {d_out}, batch_size {batch_size}, context_length {context_length}\")\n",
    "mha = MultiHeadAttention(d_in, d_out, heads_num=2, context_length=context_length, dropout=0.0 )\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360064"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out,num_heads, context_length, 0.0 )\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "batch_size = 8\n",
    "context_len = 1024\n",
    "embed_dim = 768\n",
    "embeddings = torch.randn((batch_size, context_len, embed_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "tensor([[[ 1.3391e+00,  2.0517e-01, -1.6879e+00,  ..., -4.2419e-01,\n",
      "          -5.8824e-02,  7.8626e-01],\n",
      "         [ 4.0166e-01, -2.8328e-01, -7.3094e-01,  ...,  5.2304e-01,\n",
      "           2.2982e+00,  6.3116e-01],\n",
      "         [ 5.2773e-01,  6.7984e-02, -3.2776e-01,  ..., -2.8288e-01,\n",
      "          -1.5578e+00, -8.6155e-01],\n",
      "         ...,\n",
      "         [ 2.9147e+00, -3.2614e-02, -6.2381e-01,  ...,  9.1058e-01,\n",
      "          -1.2182e+00, -4.7430e-02],\n",
      "         [ 4.2607e-01, -3.5098e-01, -1.3139e+00,  ...,  1.1188e+00,\n",
      "           1.6521e+00,  1.0859e+00],\n",
      "         [ 2.6405e-01,  8.3405e-01,  1.4404e+00,  ..., -8.5109e-01,\n",
      "          -1.4092e+00, -1.7833e-01]],\n",
      "\n",
      "        [[-1.4454e+00, -2.7590e+00,  3.8863e-01,  ...,  2.4145e-01,\n",
      "           3.2685e-02, -7.5191e-02],\n",
      "         [-8.1008e-01,  8.0733e-01,  1.0608e-01,  ..., -3.7774e-01,\n",
      "          -9.7854e-01,  7.4685e-01],\n",
      "         [-2.1887e-01,  2.5251e-01, -2.1778e-01,  ..., -1.4378e+00,\n",
      "           1.7645e-01, -1.0513e+00],\n",
      "         ...,\n",
      "         [-1.5988e-01, -9.7716e-01, -2.9663e-02,  ...,  1.3907e+00,\n",
      "           1.6214e+00,  1.1305e+00],\n",
      "         [-6.6766e-01,  3.1073e-01, -4.9830e-01,  ...,  6.7014e-01,\n",
      "           1.6780e+00, -1.1158e+00],\n",
      "         [-7.8861e-01, -1.9323e-01,  7.7176e-01,  ...,  8.8471e-01,\n",
      "           6.6951e-01,  1.2793e+00]],\n",
      "\n",
      "        [[-5.4533e-01, -3.8080e-01,  1.2217e+00,  ...,  1.6120e+00,\n",
      "          -9.8442e-01,  1.1104e+00],\n",
      "         [-1.4052e+00,  1.3569e+00, -3.6571e-01,  ...,  7.5925e-01,\n",
      "          -1.0827e-01,  1.0647e-01],\n",
      "         [ 1.0045e+00,  2.7479e-02, -9.8941e-01,  ...,  1.4866e-01,\n",
      "          -1.2348e+00, -7.6908e-01],\n",
      "         ...,\n",
      "         [-1.3936e-03, -6.6267e-01,  7.0023e-01,  ...,  3.1552e-01,\n",
      "           1.2184e+00,  2.5814e-01],\n",
      "         [-4.0462e-01, -8.0045e-02,  1.0131e+00,  ..., -2.0979e-01,\n",
      "          -5.8979e-01, -6.9788e-01],\n",
      "         [-1.3689e+00,  1.1941e-01,  1.2376e+00,  ..., -8.6775e-01,\n",
      "           1.8016e-02,  1.1177e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7190e-01,  1.3036e+00,  4.4683e-01,  ..., -7.6898e-01,\n",
      "           6.0703e-01,  6.5540e-01],\n",
      "         [-1.5456e+00,  4.0635e-01,  1.9964e+00,  ...,  1.3820e+00,\n",
      "          -1.5170e+00, -6.7899e-01],\n",
      "         [ 4.3889e-01,  1.4374e+00, -1.5895e-01,  ...,  1.5190e+00,\n",
      "          -1.7065e+00,  1.8714e+00],\n",
      "         ...,\n",
      "         [-5.8934e-02, -1.3231e+00, -1.0250e+00,  ..., -1.4726e+00,\n",
      "          -1.4736e-01, -5.8040e-01],\n",
      "         [-1.5001e-01, -1.4815e+00, -7.4577e-01,  ..., -9.4683e-01,\n",
      "          -1.1399e+00, -5.6562e-01],\n",
      "         [-2.3687e+00,  1.5949e-01, -3.0142e-01,  ...,  8.6003e-01,\n",
      "           9.0995e-03,  8.1712e-01]],\n",
      "\n",
      "        [[ 1.4569e+00,  5.3859e-01,  1.7472e+00,  ...,  5.2734e-02,\n",
      "          -6.6265e-01,  2.1897e-01],\n",
      "         [-4.2676e-01, -3.4426e-02, -2.0840e-01,  ..., -3.6739e-01,\n",
      "          -2.2836e+00,  2.6278e-01],\n",
      "         [ 9.6080e-01,  3.8789e-01,  8.4779e-01,  ...,  5.8705e-01,\n",
      "           1.0621e-01,  3.0692e+00],\n",
      "         ...,\n",
      "         [ 4.2757e-01, -1.6302e+00,  6.0339e-01,  ...,  3.9975e-01,\n",
      "          -4.8443e-01, -1.8655e+00],\n",
      "         [-5.1326e-01,  1.3672e-01, -2.0790e+00,  ..., -6.4039e-01,\n",
      "          -1.8718e+00,  3.2457e-02],\n",
      "         [ 3.3781e-01, -3.9428e-01,  1.3884e+00,  ..., -1.1177e-01,\n",
      "          -9.4029e-01,  4.7207e-02]],\n",
      "\n",
      "        [[ 3.5377e-02,  4.2982e-01,  1.0568e+00,  ..., -4.7999e-01,\n",
      "           1.8612e-01, -6.4625e-01],\n",
      "         [ 7.1341e-02, -1.4286e+00, -7.5347e-01,  ..., -1.8165e+00,\n",
      "          -8.0011e-01,  5.3300e-01],\n",
      "         [ 5.8435e-01,  9.7739e-01, -7.1008e-01,  ..., -1.3731e-01,\n",
      "          -3.3079e-01, -6.3675e-01],\n",
      "         ...,\n",
      "         [-1.6939e+00,  4.5833e-01, -2.9992e-01,  ...,  1.2478e+00,\n",
      "           9.1314e-01, -1.5466e-01],\n",
      "         [ 7.4272e-01,  2.7961e-01, -8.3932e-01,  ...,  6.4104e-01,\n",
      "          -3.8691e-01, -7.3390e-02],\n",
      "         [ 1.4136e+00,  1.1805e+00,  6.2847e-01,  ..., -6.4756e-01,\n",
      "          -1.1573e-01, -8.8555e-01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
